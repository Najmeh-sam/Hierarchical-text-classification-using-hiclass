{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa25ea8b-3dda-4d6e-853f-0db286b6cf2d",
   "metadata": {},
   "source": [
    "# WoAG Coder Capability - HiClass Model Builder\n",
    "## An Executable Notebook to Try Different HiClass Features \n",
    "\n",
    "## Najmeh Samadiani (MINDS/DESMB/MDSD)\n",
    "\n",
    "This is a running notebook to get the user inputs and pass them to different HiClass-based modules. They include three base classifiers: LogesticRegression, Random Forest, and Support Vector Machine (SGDClassifier).\n",
    "\n",
    "You are able to choose one of the two real-world datasets for your tests: the product reviews from Kaggle and the houshold data from Saudi Arabia Statistics (the latter is still under development).\n",
    "\n",
    "Feel free to raise any issues/questions through my email: najmeh.samadiani@abs.gov.au\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bed15-e1b2-4a55-8c0f-9a6ce537e07c",
   "metadata": {},
   "source": [
    "#### DGB cell - running a Processing Job\n",
    "https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/scikit_learn_data_processing_and_model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203910e6-ba36-46b9-99e7-4d6b9a750295",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "# !pip install thinc\n",
    "# √ó Failed to build installable wheels for some pyproject.toml based projects\n",
    "#       ‚ï∞‚îÄ> thinc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785342d6-4c83-4386-9e56-9ed97cb550b7",
   "metadata": {},
   "source": [
    "###___________________________________________________________________________________________________________###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8920da2-1e59-44c0-bb0d-3f70358a9218",
   "metadata": {},
   "source": [
    "DGB comment - created virtual environment before installing requirements.txt\n",
    "```\n",
    "virtualenv venv\n",
    "source venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735641a2-fb65-4a90-b11d-929eb57cf164",
   "metadata": {},
   "source": [
    "DGB comment - Upgraded pip\n",
    "```\n",
    "pip install --upgrade pip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "262af768-71f4-4880-a157-583d5e796203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/git_codecommit/minds_coder_capability_hiclass'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7a04ce3-15e7-462a-a04e-8b2c546c50fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting install_rqts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile install_rqts.py\n",
    "#Check the requirements file to ensure all necessary libraries are installed\n",
    "import sys\n",
    "sys.path.append(\"./helper\")\n",
    "from helper import install_requirements #helper.helper\n",
    "# find the requirements.txt file in the main folder\n",
    "install_requirements('./requirements.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355fd784-7e6f-4d01-a6a1-3d4a5e835d3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß≠ Model Configuration Panel\n",
    "Please use the configuration form below to define how you'd like your hierarchical classification model to behave. This panel allows you to control every major component of the pipeline, including:\n",
    "\n",
    "üßæ **Dataset Selection**\n",
    "- Choose Dataset: Select which dataset you'd like to use (dataset1, or dataset2) - 'dataset2 is still under development'\n",
    "- Label Columns: Enter the column names that represent the hierarchical labels for your selected dataset (e.g., Cat1, Cat2, Cat3).\n",
    "\n",
    "üìù **Text Feature Selection**\n",
    "\n",
    "Text Column - Choose which column(s) to use for feature extraction:\n",
    "- Title: Use only the title column.\n",
    "- Text: Use only the text/content column.\n",
    "- Both: Concatenate Title and Text for richer input.\n",
    "\n",
    "‚ú® **TF-IDF Preprocessing Settings**\n",
    "\n",
    "Configure the text-to-numeric transformation:\n",
    "\n",
    "- Minimum / Maximum Document Frequency: Control rare or overly common term filtering.\n",
    "- N-gram Range: Specify the size of text chunks to analyze (e.g., unigrams or bigrams).\n",
    "- Stop Words: Choose a stop-word list (e.g., English) or disable it.\n",
    "\n",
    "---\n",
    "\n",
    "üß† **HiClass Strategies**\n",
    "\n",
    "HiClass provides multiple strategies for building hierarchical classifiers. Each strategy controls how base classifiers are positioned relative to the hierarchy structure:\n",
    "\n",
    "üîπ `lcppn` ‚Äî Local Classifier Per Parent Node (default)\n",
    "- Trains a separate classifier at **each parent node** to distinguish between its direct children.\n",
    "- **Advantages**: Efficient and focused. Classifiers learn from local child distributions only.\n",
    "- **Recommended for**: Large or deep hierarchies where different parent nodes have distinct structures.\n",
    "\n",
    "üîπ `lcpn` ‚Äî Local Classifier Per Node\n",
    "- Trains a classifier at **every node**, including leaf nodes (Can result in many small classifiers, increasing complexity).\n",
    "- **Advantages**: Fine-grained control and potentially better local precision.\n",
    "- **Recommended for**: Irregular hierarchies or custom tree control.\n",
    "\n",
    "üîπ `lcpl` ‚Äî Local Classifier Per Level\n",
    "- Trains one classifier per **hierarchical level** (e.g., one for top-level, one for mid-level, etc.).\n",
    "- **Advantages**: Simple to train. Less memory intensive. Easy to interpret.\n",
    "- **Recommended for**: Shallow hierarchies or where level-wise accuracy is prioritized.\n",
    "\n",
    "---\n",
    "\n",
    "üß† **Classifier Settings**\n",
    "\n",
    "Choose which base classifier(s) you'd like to use (**you have an option to choose all three in a once**):\n",
    "\n",
    "- SGD: Linear model with SVM-style or logistic loss\n",
    "- LogisticRegression: Classical log-linear model\n",
    "- RandomForest: Ensemble-based non-linear classifier\n",
    "\n",
    "Each classifier section includes parameters like:\n",
    "\n",
    "- Number of estimators, maximum iterations, maximum depth, and criterion for Random Forest\n",
    "- Max iterations, Loss function (hinge/log) and SVM class weight for SGD\n",
    "\n",
    "üß™ **Model Calibration & Probabilities**\n",
    "\n",
    "Calibration/Probability Control:\n",
    "- none: No probability output or calibration\n",
    "- calibration only: Apply isotonic/beta/ivap on raw model scores\n",
    "- probability only: Enable probability combination (e.g., geometric)\n",
    "- both: Use both calibrated scores and probability combination\n",
    "\n",
    "- Calibration Method: Choose from isotonic, beta, ivap, or cvap.\n",
    "- Probability Combiner: Choose how probabilities are merged across hierarchy levels (e.g., geometric, arithmetic, or multiply).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096501f-a715-49a5-a409-1331a0516ad9",
   "metadata": {},
   "source": [
    "## break into 3 scripts\n",
    "1. install_rqmts.py = loading requirements.txt\n",
    "2. dynamic_vals.py  = create dynamic values - dynamic_vals.py . DGB - we could add piece to feed Dynamic values into process job script\n",
    "3. processjob.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118e06a-1c8e-4437-acb7-80f8024f99bd",
   "metadata": {},
   "source": [
    "### DGB cell - problems with openpyxl module - put into requirements.txt - maybe even install this again on command line \n",
    "```\n",
    "pip install openpxyl\n",
    "pip show openpxyl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4f3623-4c0c-4acd-b7e6-beefd1c31738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>üõ†Ô∏è Model Configuration Inputs</h2><p>Please fill in the configuration below to build and train your HiClass model.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ab7ab668cd4af597eaa86e287ad33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Choose Dataset</b>'), Dropdown(description='Dataset', options=('dataset1', 'data‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec689648e1cb4b6fa24c40d680be1a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Cat1,Cat2,Cat3', description='Labels for dataset1'),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edff309aa2545dd8cc3a1d914b3a3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Text Column</b>'), Dropdown(options=('Title', 'Text', 'Both'), value='Title')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b33376ffdc40c0979a3f19ccb11aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Min Df</b>'), IntText(value=1)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b24b19666647bc84b57da4637c382a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Max Df</b>'), FloatText(value=0.5)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5faeb61fd44a70884b0437e04b10a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Max Features</b>'), IntText(value=50000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbf72d448524eff940f3af07f0101fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Stop Words</b>'), Dropdown(options=('english', None), value='english')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90f9175f1634fcd8f3c21ff8c0ce3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Ngram Min</b>'), IntText(value=1)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1fd703eed54d27907e85040f2f6689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Ngram Max</b>'), IntText(value=2)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e51359b5bd41d6916ab1fdc35f65a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Select Base Classifiers (multi-select)</b>'), SelectMultiple(index=(0, 1, 2), op‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec66a01cccb451fa7007829d04093a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Calibration/Probability Control</b>'), Dropdown(index=3, options=('none', 'proba‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274d6588e21a4ea7bd51810d7ce78af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<b>Calibration Method</b>'), Dropdown(options=('isotonic', 'beta', '‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8565892a10649f4820f083a65009a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Hiclass Strategy</b>'), Dropdown(options=('lcppn', 'lcpn', 'lcpl'), value='lcppn‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622ace9513dd4375bdb34faf2b7122a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Sgd Loss</b>'), Dropdown(options=('hinge', 'log_loss'), value='hinge')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a717800d16493dbefb10fd11360656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Sgd Class Weight</b>'), Dropdown(options=('balanced', None), value='balanced')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc9e3c8506f4baa97757a8712e4c0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Sgd Max Iter</b>'), IntText(value=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c84c1bfdde475ab2d35e910446558e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Random State</b>'), IntText(value=0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1e4a7ef46249d5bc794a53ad4181b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Rf Estimators</b>'), IntText(value=100)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f980a0de3884d65a66c23d6efab0e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Rf Max Depth</b>'), IntText(value=0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6229ff13ff704f1d9f4aa60875c8384f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Rf Criterion</b>'), Dropdown(options=('gini', 'entropy'), value='gini')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%writefile dynamic_vals.py\n",
    "import sys\n",
    "# Adding the relevant folders to the python path\n",
    "sys.path.append(\"./modules\")\n",
    "\n",
    "from helper.display_utils import build_model_configuration_widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<h2>üõ†Ô∏è Model Configuration Inputs</h2><p>Please fill in the configuration below to build and train your HiClass model.</p>\"))\n",
    "# print(\"Please fill in the configuration below to build and train your HiClass model.\")\n",
    "\n",
    "dynamic_values = build_model_configuration_widgets()\n",
    "\n",
    "# DGB - Save dynamic values as json \n",
    "# import yaml\n",
    "# config_filepath = \"./dynamic_values.yaml\"\n",
    "# config=open(\"dynamic_values.yaml\",\"w\")\n",
    "# yaml.dump(dynamic_values,config)\n",
    "# print(\"YAML config file saved.\")\n",
    "# config.close()\n",
    "\n",
    "# import json\n",
    "# dv_filepath = \"./dynamic_values.json\"\n",
    "# with open(dv_filepath, \"w\") as json_file:\n",
    "#     json.dump(dynamic_values, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40059330-c03c-479f-9c1f-b91a409a2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dynamic values as json - cannot capture new dynamic values entered in widgets if this code block is in cell above\n",
    "import json\n",
    "with open(\"./dynamic_values.json\", \"w\") as json_file:\n",
    "    json.dump(dynamic_values, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e146c399-00b0-48ef-9ba1-ed794717a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# config_filepath = \"./dynamic_values.yaml\"\n",
    "# config=open(\"dynamic_values.yaml\",\"w\")\n",
    "# yaml.dump(dynamic_values,config)\n",
    "# print(\"YAML config file saved.\")\n",
    "# config.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5e6cd-2de1-486d-bcf9-a1b2669f12b6",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "Once you‚Äôve configured your model using above panel, click Run to execute this cell for:\n",
    "\n",
    "- Train selected models\n",
    "\n",
    "- Evaluate performance\n",
    "\n",
    "- View confusion matrices\n",
    "\n",
    "- Save trained pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "368aa59d-5938-4de2-a05c-237e32be4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the models based on the given input\n",
    "# !pip install openpyxl #hiclass \n",
    "import hiclass\n",
    "from run_workflows import (\n",
    "    run_logreg_workflow_from_widgets,\n",
    "    run_rf_workflow_from_widgets,\n",
    "    run_sgd_workflow_from_widgets \n",
    ")\n",
    "\n",
    "# Dictionary to collect trained models\n",
    "trained_models = {}\n",
    "\n",
    "# Iterate over selected classifiers from widgets\n",
    "for classifier_name in dynamic_values['base_classifiers']:\n",
    "    clf = classifier_name.lower()\n",
    "\n",
    "    if clf == 'logisticregression':\n",
    "        print(\"‚öôÔ∏è Running Logistic Regression workflow...\")\n",
    "        model = run_logreg_workflow_from_widgets(dynamic_values)\n",
    "        trained_models['LogisticRegression'] = model\n",
    "\n",
    "    elif clf == 'randomforest':\n",
    "        print(\"üå≤ Running Random Forest workflow...\")\n",
    "        model = run_rf_workflow_from_widgets(dynamic_values)\n",
    "        trained_models['RandomForest'] = model\n",
    "\n",
    "    elif clf == 'sgd':\n",
    "        print(\"‚öôÔ∏è Running SGD workflow...\")\n",
    "        model = run_sgd_workflow_from_widgets(dynamic_values)\n",
    "        trained_models['SGD'] = model\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùì Unknown classifier selected: {classifier_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53e328b1-d016-4f17-a5a2-785a33e62b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting processjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile processjob.py\n",
    "\n",
    "## DGB code\n",
    "# config = yaml.safe_load(config_filepath)\n",
    "# print(\"Config file of Dynamic values file loaded.\")\n",
    "with open(\"./dynamic_values.json\", 'r') as file:\n",
    "    dynamic_values = json.load(file)\n",
    "print(\"the dynamic values are\",dynamic_values)\n",
    "\n",
    "# Executing the models based on the given input\n",
    "# !pip install openpyxl #hiclass \n",
    "import hiclass\n",
    "import sys\n",
    "sys.path.append(\"./helper\")\n",
    "sys.path.append(\"./modules\")\n",
    "from modules.run_workflows import (\n",
    "    run_logreg_workflow_from_widgets,\n",
    "    run_rf_workflow_from_widgets,\n",
    "    run_sgd_workflow_from_widgets \n",
    ")\n",
    "\n",
    "# Dictionary to collect trained models\n",
    "trained_models = {}\n",
    "\n",
    "# Iterate over selected classifiers from widgets\n",
    "for classifier_name in dynamic_values['base_classifiers']:\n",
    "    clf = classifier_name.lower()\n",
    "\n",
    "    if clf == 'logisticregression':\n",
    "        print(\"‚öôÔ∏è Running Logistic Regression workflow...\")\n",
    "        model = run_logreg_workflow_from_widgets(dynamic_values)\n",
    "        trained_models['LogisticRegression'] = model\n",
    "\n",
    "    elif clf == 'randomforest':\n",
    "        print(\"üå≤ Running Random Forest workflow...\")\n",
    "        model = run_rf_workflow_from_widgets(dynamic_values)\n",
    "        trained_models['RandomForest'] = model\n",
    "\n",
    "    elif clf == 'sgd':\n",
    "        print(\"‚öôÔ∏è Running SGD workflow...\")\n",
    "        model = run_sgd_workflow_from_widgets(dynamic_values)\n",
    "        trained_models['SGD'] = model\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùì Unknown classifier selected: {classifier_name}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_and_compare_models(trained_models, X_test_text, y_test, output_path=\"outputs/model_comparison.xlsx\"):\n",
    "    \"\"\"\n",
    "    Evaluates all trained models, compares their metrics, and saves results to Excel.\n",
    "\n",
    "    Parameters:\n",
    "    - trained_models: dict of {model_name: model_instance}\n",
    "    - X_test_text: text input for prediction\n",
    "    - y_test: true hierarchical labels\n",
    "    - output_path: where to save the comparison Excel file\n",
    "\n",
    "    Returns:\n",
    "    - summary_df: DataFrame of all model metrics\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        for model_name, model in trained_models.items():\n",
    "            print(f\"üîç Evaluating {model_name}...\")\n",
    "            model.predict(X_test_text)\n",
    "            metrics = model.evaluate(y_test)\n",
    "\n",
    "            flat_metrics = {\n",
    "                'model': model_name,\n",
    "                'hierarchical_f1': metrics['hierarchical_f1'],\n",
    "                'hierarchical_precision': metrics['hierarchical_precision'],\n",
    "                'hierarchical_recall': metrics['hierarchical_recall'],\n",
    "                'exact_path_accuracy': metrics['exact_path_accuracy']\n",
    "            }\n",
    "\n",
    "            # Include level-wise accuracy\n",
    "            for level, acc in metrics['level_accuracy'].items():\n",
    "                flat_metrics[f'accuracy_{level}'] = acc\n",
    "\n",
    "            all_metrics.append(flat_metrics)\n",
    "\n",
    "            # Save full metric details as a sheet\n",
    "            pd.DataFrame([flat_metrics]).to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "    summary_df = pd.DataFrame(all_metrics)\n",
    "    summary_df.to_excel(output_path, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "    print(f\"‚úÖ Evaluation comparison saved to: {output_path}\")\n",
    "    return summary_df\n",
    "\n",
    "# This assumes all models were trained and test data is ready\n",
    "from helper.helper import prepare_text_and_labels, build_dataset_sources_and_labels, build_dataset_map\n",
    "\n",
    "# Step 1: Get test data\n",
    "source_dict, label_dict = build_dataset_sources_and_labels(dynamic_values)\n",
    "dataset_map = build_dataset_map(source_dict, label_dict)\n",
    "_, X_test_text, _, y_test = prepare_text_and_labels(dynamic_values, dataset_map)\n",
    "\n",
    "# Step 2: Evaluate all\n",
    "results_df = evaluate_and_compare_models(trained_models, X_test_text, y_test)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d848fe2e-9bf7-4780-bc36-4569c3723f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating SGD...\n",
      "üîç Evaluating LogisticRegression...\n",
      "üîç Evaluating RandomForest...\n",
      "‚úÖ Evaluation comparison saved to: outputs/model_comparison.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>hierarchical_f1</th>\n",
       "      <th>hierarchical_precision</th>\n",
       "      <th>hierarchical_recall</th>\n",
       "      <th>exact_path_accuracy</th>\n",
       "      <th>accuracy_Cat1</th>\n",
       "      <th>accuracy_Cat2</th>\n",
       "      <th>accuracy_Cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.902904</td>\n",
       "      <td>0.902904</td>\n",
       "      <td>0.902904</td>\n",
       "      <td>0.863365</td>\n",
       "      <td>0.940747</td>\n",
       "      <td>0.897524</td>\n",
       "      <td>0.870440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.875967</td>\n",
       "      <td>0.875967</td>\n",
       "      <td>0.875967</td>\n",
       "      <td>0.815167</td>\n",
       "      <td>0.936436</td>\n",
       "      <td>0.869335</td>\n",
       "      <td>0.822131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.887096</td>\n",
       "      <td>0.887096</td>\n",
       "      <td>0.887096</td>\n",
       "      <td>0.847336</td>\n",
       "      <td>0.929582</td>\n",
       "      <td>0.879615</td>\n",
       "      <td>0.852089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  hierarchical_f1  hierarchical_precision  \\\n",
       "0                 SGD         0.902904                0.902904   \n",
       "1  LogisticRegression         0.875967                0.875967   \n",
       "2        RandomForest         0.887096                0.887096   \n",
       "\n",
       "   hierarchical_recall  exact_path_accuracy  accuracy_Cat1  accuracy_Cat2  \\\n",
       "0             0.902904             0.863365       0.940747       0.897524   \n",
       "1             0.875967             0.815167       0.936436       0.869335   \n",
       "2             0.887096             0.847336       0.929582       0.879615   \n",
       "\n",
       "   accuracy_Cat3  \n",
       "0       0.870440  \n",
       "1       0.822131  \n",
       "2       0.852089  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_and_compare_models(trained_models, X_test_text, y_test, output_path=\"outputs/model_comparison.xlsx\"):\n",
    "    \"\"\"\n",
    "    Evaluates all trained models, compares their metrics, and saves results to Excel.\n",
    "\n",
    "    Parameters:\n",
    "    - trained_models: dict of {model_name: model_instance}\n",
    "    - X_test_text: text input for prediction\n",
    "    - y_test: true hierarchical labels\n",
    "    - output_path: where to save the comparison Excel file\n",
    "\n",
    "    Returns:\n",
    "    - summary_df: DataFrame of all model metrics\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        for model_name, model in trained_models.items():\n",
    "            print(f\"üîç Evaluating {model_name}...\")\n",
    "            model.predict(X_test_text)\n",
    "            metrics = model.evaluate(y_test)\n",
    "\n",
    "            flat_metrics = {\n",
    "                'model': model_name,\n",
    "                'hierarchical_f1': metrics['hierarchical_f1'],\n",
    "                'hierarchical_precision': metrics['hierarchical_precision'],\n",
    "                'hierarchical_recall': metrics['hierarchical_recall'],\n",
    "                'exact_path_accuracy': metrics['exact_path_accuracy']\n",
    "            }\n",
    "\n",
    "            # Include level-wise accuracy\n",
    "            for level, acc in metrics['level_accuracy'].items():\n",
    "                flat_metrics[f'accuracy_{level}'] = acc\n",
    "\n",
    "            all_metrics.append(flat_metrics)\n",
    "\n",
    "            # Save full metric details as a sheet\n",
    "            pd.DataFrame([flat_metrics]).to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "    summary_df = pd.DataFrame(all_metrics)\n",
    "    summary_df.to_excel(output_path, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "    print(f\"‚úÖ Evaluation comparison saved to: {output_path}\")\n",
    "    return summary_df\n",
    "\n",
    "# This assumes all models were trained and test data is ready\n",
    "from helper import prepare_text_and_labels, build_dataset_sources_and_labels, build_dataset_map\n",
    "\n",
    "# Step 1: Get test data\n",
    "source_dict, label_dict = build_dataset_sources_and_labels(dynamic_values)\n",
    "dataset_map = build_dataset_map(source_dict, label_dict)\n",
    "_, X_test_text, _, y_test = prepare_text_and_labels(dynamic_values, dataset_map)\n",
    "\n",
    "# Step 2: Evaluate all\n",
    "results_df = evaluate_and_compare_models(trained_models, X_test_text, y_test)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5ba62-901e-4818-8162-9838a560779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical_f1\thierarchical_precision\thierarchical_recall\tall the same "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
